{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e3af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07305d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_csv('Input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d5021e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extract(row):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup as bs\n",
    "    page = requests.get(row['URL'])\n",
    "    soup = bs(page.content)\n",
    "    text_list = [j.text.replace(u'\\xa0', u' ') for j in soup.find_all(['h1','p','ol'])]\n",
    "    text_file = open(str(row['URL_ID'])+'.txt','w',encoding = 'utf-8')\n",
    "    my_string = ' '.join(text_list)\n",
    "    text_file.write(my_string)\n",
    "    text_file.close()\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15798ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZE COLLECTED TEXT\n",
    "def create_tokens(row,c):\n",
    "    with open(str(row['URL_ID'])+'.txt', encoding='utf-8') as file:\n",
    "        all_text = file.read()\n",
    "    word_tokens = nltk.word_tokenize(all_text)\n",
    "    punctuations = ['?','!',',','.',';','&','“','”','’']\n",
    "    word_tokens = [item.lower() for item in word_tokens if item not in punctuations]\n",
    "    sent_tokens = nltk.sent_tokenize(all_text)\n",
    "    if (c=='word'):\n",
    "        return word_tokens\n",
    "    elif (c=='sentence'):\n",
    "        return sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe43c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SYLLABLE COUNT\n",
    "def syllable_count(word):\n",
    "    count = len(re.findall('[aeiou]',word.lower()))\n",
    "    if re.search('ed|ed$',word.lower()):\n",
    "        count-=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd923149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOPWORDS LIST\n",
    "name_list = ['Auditor', 'Currencies', 'DatesandNumbers', 'Generic', 'GenericLong', 'Geographic', 'Names']\n",
    "big_stword_list = []\n",
    "\n",
    "for i in name_list:\n",
    "    with open('./StopWords/StopWords_'+i+'.txt') as file:\n",
    "        st_text = file.read()\n",
    "    if (i != 'GenericLong'):\n",
    "        something = nltk.word_tokenize(st_text)\n",
    "        big_stword_list += [item.lower() for item in something if item.isupper()]\n",
    "    else:\n",
    "        big_stword_list += nltk.word_tokenize(st_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b0c1b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#POSITIVE WORDS AND NEGATIVE WORDS DICTIONARY (LIST)\n",
    "with open('./MasterDictionary/positive-words.txt') as file:\n",
    "    pos_dict = file.read()\n",
    "pos_dict = [item for item in nltk.word_tokenize(pos_dict) if item not in big_stword_list]\n",
    "\n",
    "with open('./MasterDictionary/negative-words.txt') as file:\n",
    "    neg_dict = file.read()\n",
    "neg_dict = [item for item in nltk.word_tokenize(neg_dict) if item not in big_stword_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b52af68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimental_analysis(row):\n",
    "    #stopwords from StopWords Lists removed from tokenized words\n",
    "    word_tokens = create_tokens(row,'word')\n",
    "    if len(word_tokens) == 0:\n",
    "        return row\n",
    "    more_cleaned_words = [item for item in word_tokens if item not in big_stword_list]\n",
    "    #scores\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    for i in more_cleaned_words:\n",
    "        if i in pos_dict:\n",
    "            pos_score+=1\n",
    "        elif i in neg_dict:\n",
    "            neg_score-=1\n",
    "    neg_score*=-1\n",
    "    row['POSITIVE SCORE'] = pos_score\n",
    "    row['NEGATIVE SCORE'] = neg_score\n",
    "\n",
    "    polarity_score = (pos_score - neg_score)/((pos_score + neg_score)+0.000001)\n",
    "    row['POLARITY SCORE'] = polarity_score\n",
    "\n",
    "    subjectivity_score = (pos_score + neg_score)/(len(more_cleaned_words)+0.000001)\n",
    "    row['SUBJECTIVITY SCORE'] = subjectivity_score\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "306cc020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_analysis(row):\n",
    "    word_tokens = create_tokens(row,'word')\n",
    "    if len(word_tokens) == 0:\n",
    "        return row\n",
    "    sent_tokens = create_tokens(row,'sentence')\n",
    "    avg_sent_length = len(word_tokens)/len(sent_tokens)\n",
    "    row['AVG SENTENCE LENGTH'] = avg_sent_length\n",
    "\n",
    "    complex_count = 0\n",
    "    for i in word_tokens:\n",
    "        if syllable_count(i) > 2:\n",
    "            complex_count+=1\n",
    "    percent_complex = complex_count/len(word_tokens)\n",
    "    row['PERCENTAGE OF COMPLEX WORDS'] = percent_complex\n",
    "\n",
    "    fog_index = 0.4*(avg_sent_length + percent_complex)\n",
    "    row['FOG INDEX'] = fog_index\n",
    "\n",
    "    #avg number of words per sentence is same as avg sentence length\n",
    "    row['AVG NUMBER OF WORDS PER SENTENCE'] = avg_sent_length\n",
    "\n",
    "    row['COMPLEX WORD COUNT'] = complex_count\n",
    "\n",
    "    #words cleaned with nltk stopwords package\n",
    "    from nltk.corpus import stopwords\n",
    "    stops = set(stopwords.words('english'))\n",
    "    cleaned_words = [item for item in word_tokens if item not in stops]\n",
    "    row['WORD COUNT'] = len(cleaned_words)\n",
    "\n",
    "    #avg syllable count since each row has multiple words\n",
    "    total_syllable = 0\n",
    "    for i in word_tokens:\n",
    "        total_syllable += syllable_count(i)\n",
    "    row['SYLLABLE PER WORD'] = total_syllable/len(word_tokens)\n",
    "\n",
    "    #Personal pronouns count\n",
    "    pronouns = ['I','we','my','ours','us','We','My','Ours','Us','WE','MY','OURS']\n",
    "    pro_count = len([i for i in word_tokens if i in pronouns])\n",
    "    row['PERSONAL PRONOUNS'] = pro_count\n",
    "\n",
    "    #avg word length\n",
    "    len_list = sum([len(i) for i in cleaned_words])\n",
    "    row['AVG WORD LENGTH'] = len_list/len(cleaned_words)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac73986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpute_data = input_data.apply(data_extract, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "596d3e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_data.apply(sentimental_analysis, axis = 1)\n",
    "input_data = input_data.apply(readability_analysis, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "580fd013",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = input_data[['URL_ID','URL','POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH']]\n",
    "result.to_excel('OUTPUT.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
